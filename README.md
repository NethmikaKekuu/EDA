Churn Modelling â€” Data Preprocessing Pipeline

A step-by-step machine learning preprocessing pipeline built on the **Bank Churn Modelling dataset**. This project covers the full journey from raw, messy data to a clean, encoded, and scaled dataset ready for model training.

---

## ğŸ“‹ Project Overview

The goal is to preprocess customer banking data to predict churn (whether a customer will leave the bank). Each notebook in this project handles one stage of the preprocessing pipeline:

| Step | Notebook | Description |
|------|----------|-------------|
| 0 | `0_handle_missing_values.ipynb` | Impute missing ages with mean/median; predict missing genders using an LLM (Groq API) |
| 1 | `1_handling_outliers.ipynb` | Detect outliers using KDE plots, box plots, 3-sigma rule, and IQR method |
| 2 | `2_feature_binning.ipynb` | Bin `CreditScore` into ordinal categories (Poor â†’ Excellent) |
| 3 | `3_feature_encoding.ipynb` | One-hot encode nominal columns (Geography, Gender); label encode ordinal column (CreditScoreBins) |
| 4 | `4_feature_scaling.ipynb` | Standardize continuous features (Age, Tenure, Balance, EstimatedSalary) using Z-score scaling |

### Data Flow

```
ChurnModelling.csv  (raw)
        â†“  0_handle_missing_values
ChurnModelling_Missing_values_handled.csv
        â†“  1_handling_outliers
ChurnModelling_Outliers_Handled.csv
        â†“  2_feature_binning
ChurnModelling_Binning_Applied.csv
        â†“  3_feature_encoding
ChurnModelling_Encoded.csv
        â†“  4_feature_scaling
ChurnModelling_Final.csv  âœ… model-ready
```

---

## ğŸ What is Anaconda?

**Anaconda** is a free distribution of Python that comes pre-packaged with hundreds of data science libraries (NumPy, pandas, scikit-learn, etc.) and the **conda** package manager. Instead of manually installing Python and every library one by one, Anaconda gives you everything in one installer.

Think of it like this:
- **Python alone** = a kitchen with no equipment
- **Anaconda** = a fully equipped kitchen, ready to cook

Download it here: https://www.anaconda.com/download

---

## ğŸ“¦ What is a Virtual Environment?

A **virtual environment** is an isolated Python workspace. It has its own Python version and its own set of installed packages, completely separate from every other project on your machine.

**Why does this matter?**

Imagine two projects:
- Project A needs `pandas==1.5`
- Project B needs `pandas==3.0`

Without environments, installing one would break the other. With environments, each project lives in its own bubble and never interferes with anything else.

```
Your computer
â”œâ”€â”€ env_project_a/    â† pandas 1.5, scikit-learn 1.0
â”œâ”€â”€ env_project_b/    â† pandas 3.0, scikit-learn 1.8
â””â”€â”€ env_this_project/ â† exact versions in requirements.txt
```

---

## ğŸš€ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2. Create a conda environment

```bash
conda create -n bprmls python=3.11
```

This creates a fresh isolated environment named `bprmls` running Python 3.11.

### 3. Activate the environment

```bash
conda activate bprmls
```

Your terminal prompt will change to show `(bprmls)` â€” this means you are now inside the environment. Any packages you install from this point go into this environment only.

### 4. Install dependencies

```bash
pip install -r requirments.txt
```

### 5. Set up your `.env` file (required for notebook 0)

Notebook `0_handle_missing_values.ipynb` uses the **Groq API** to predict missing gender values from customer names. You need a free API key from https://console.groq.com.

Create a file named `.env` in the root of the project:

```
GROQ_API_KEY=your_api_key_here
```

> âš ï¸ Never commit your `.env` file to GitHub. It is already listed in `.gitignore`.

### 6. Place the raw data

Make sure the raw dataset is placed at:

```
data/raw/ChurnModelling.csv
```

The `data/processed/` folder will be populated automatically as you run the notebooks in order.

### 7. Open Jupyter and run notebooks in order

```bash
jupyter notebook
```

Run notebooks **0 â†’ 1 â†’ 2 â†’ 3 â†’ 4** in sequence. Each notebook reads the output CSV from the previous step.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ ChurnModelling.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ ChurnModelling_Missing_values_handled.csv
â”‚       â”œâ”€â”€ ChurnModelling_Outliers_Handled.csv
â”‚       â”œâ”€â”€ ChurnModelling_Binning_Applied.csv
â”‚       â”œâ”€â”€ ChurnModelling_Encoded.csv
â”‚       â””â”€â”€ ChurnModelling_Final.csv
â”œâ”€â”€ 0_handle_missing_values.ipynb
â”œâ”€â”€ 1_handling_outliers.ipynb
â”œâ”€â”€ 2_feature_binning.ipynb
â”œâ”€â”€ 3_feature_encoding.ipynb
â”œâ”€â”€ 4_feature_scaling.ipynb
â”œâ”€â”€ requirments.txt
â”œâ”€â”€ .env                  â† you create this (not committed)
â””â”€â”€ README.md
```

---

## ğŸ“š Techniques Covered

**Missing Value Handling**
- Row/column deletion
- Mean & median imputation for numerical features
- LLM-based imputation for categorical features (Groq API)

**Outlier Detection**
- KDE distribution plots
- Box plots
- 3-Sigma (empirical) rule
- IQR method

**Feature Binning**
- Custom binning of `CreditScore` into interpretable categories: `Poor / Fair / Good / Very Good / Excellent`

**Feature Encoding**
- One-hot encoding for nominal variables (`Geography`, `Gender`) using `pd.get_dummies`
- Ordinal / label encoding for `CreditScoreBins` using a manual mapping dictionary
- sklearn `OneHotEncoder` and `LabelEncoder` as an alternative approach

**Feature Scaling**
- Z-score standardization using `sklearn.preprocessing.StandardScaler` on `Age`, `Tenure`, `Balance`, `EstimatedSalary`

---

## ğŸ”§ Requirements

See `requirments.txt` for the full list. Key libraries:

- `numpy`
- `pandas`
- `scikit-learn`
- `seaborn`
- `matplotlib`
- `openai`
- `groq`
- `python-dotenv`

---

## âš ï¸ Notes

- Notebooks must be run **in order** (0 â†’ 4) since each reads the output of the previous one.
- The `.env` file is required only for notebook 0. All other notebooks work without an API key.
- The `data/processed/` folder is gitignored. Run the notebooks to regenerate the processed files.
